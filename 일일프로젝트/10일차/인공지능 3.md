## Naiva Bayes Classifier

classification 모델.

왜 이렇게 분류했나가 있고, 실제 연구에서 기본 모형으로 쓰고 있다.

### Features

성질, 데이터에 대한 디테일, ...

분류하고자 하는 데이터를 숫자로 표현해야 컴퓨터가 이해할 수 있다.

이 과정을 feature 라고 할 수 있다.

ex) CPTs

uniform distribution 과 non-uniform value

Parameter Estimation

Empirically: use training data

확률값을 계산한다!

### text

나이브 베이즈 모델은 단어의 순서를 상관없이 Bag-of-Words(몇 번 나왔는지)만 고려한다.

Overfitting 이 문제될 수 있다.

### Laplace Smoothing

오버피팅을 방지하기 위해 하는 것.

값에 무조건 + 1 씩 한다. 0 이 되는 결과는 나오지 않는다.

### Held-Out Data == Validation Data

+1 말고 +k 할 때, 이 k 를 Hyperparameter 라고 한다. 거의 모든 기계학습 모델들이 이 하이퍼파라미터를 가지고 있다.

어떻게 가장 적합한 k 를 찾아낼 수 있을까?

학습한 모델이 얼마나 좋은지 정확도는 항상 test 데이터에 대해서 한다.

### Baselines

베이스라인 설정이 가장 중요하다.

나이브 베이즈 같은 샘플이 있기도 하고, most frequent label classifier 으로 하는 weak baseline 보다는 strong 한 베이스라인을 쓴다.

### Errors

잘 안됐을 때 → 다양한 feature 를 고려해야 한다.