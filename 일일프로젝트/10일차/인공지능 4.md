## Deep Natural Language Processing(NLP)

의미를 추출해내는 것이 NLP 의 궁극적인 목표이다.

## Word Embedding

단어를 수치화, 어휘의 단어 또는 구가 실수 벡터로 매핑되는 자연 언어 처리의 언어 모델링 및 기능 학습 기술 집합의 총칭이다.(@위키백과)

### **Word Vector Representations**

단어 하나가 하나의 디멘션이 되는 것을 one-hot encoding 이라고 하는데, 단어 간의 관계를 표현할 수 없다는 단점이 있다.

그래서 나온 것이 Word Vector Representation 이다.

하나의 단어가 여러 개의 실수로 표현된다.

어떻게 관계를 포착할까? → 벡터 공간에서 위치, 방향으로 찾는다. == word embedding space

== Distribute Representations

### 워드임베딩도 **Neural Network** 이다.

Many-to-many relationship

하나의 컨셉이 여러 개의 neuron 을 가지고, 하나의 neuron 이 여러 컨셉이 있다.

Neral Network 를 통해 학습하면,  의미적, 문법적 특성을 갖게 된다.

### **Distributional Hypothesis**

여러 가지 문맥을 고려하다보면 유사한 단어들은 유사한 공간에 있게 될 것이다.

### **Word2Vec**

가장 유명한 Word Vector Representation

- **CBOW**

  문맥에 있는 단어가 주어지고, 중간에 있는 단어를 Projection

- **Skip-Gram**

  중간 단어를 알고 있을 때, 양 옆을 예측

단어, 벡터를 잘 학습한다는 것은 유사한 의미를 가진 단어들이 유사한 공간에 있고, 방향성이 적절하다는 의미이다.

### Evaluation

학습한 단어, 벡터를 어떻게 **Evaluate** 할까?

- **Word Similarity Task**

  유사도 측정

  cosine similarity 를 측정했을 때도 유사해야 한다.

- **Word Analogy Task**

  단어들 간의 관계

### **워드 임베딩의 문제들**

어떤 단어가 나오지 않았을 때는 학습할 수 없다.

데이터가 몇번 나오지 않았을 때 제대로 학습할 수 없다.

→ 다른 유닛들에 대해 임베딩해서 해결한다.

**Subword Information Skip-Gram**

단어를 쪼개서 다 모델링한다.

### **Contextualized Word Embedding**

- ELMo(Embeddings from Language Models)

  단어가 가지고 있는 의미를 잘 캡쳐하는 모델을 만들면서, 단어가 어떤 문맥에서 나왔는지 고려한다.

  단순히 가까이 있는 단어의 의미만 가져온다. 꼭 가깝다고 관련이 있는건 아니기 때문에 데이터의 시퀀스에 영향을 많이 받는다. 긴 시퀀스는 잘 학습하지 못한다.

- BERT(Bidirectional Encoder Representations from Transformers)

  어떤 단어와 가장 연관있는지는 순서가 아니라 단어에 따라 다르다.

  self-attention

  단어의 성격 자체를 데이터로부터 배워서 거리와 관계없이 연관있다는것을 포착하는 것이 Transfomer

  단어를 해석할 때 양방향으로 한다.